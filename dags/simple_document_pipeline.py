"""Simplified Airflow DAG for Document Processing Pipeline.

This DAG uses the existing API endpoints instead of importing MLOps modules.
Works out-of-the-box without additional dependencies.
"""

from datetime import datetime, timedelta
import logging
import requests
import json
from pathlib import Path

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

logger = logging.getLogger(__name__)

# Configuration
API_BASE_URL = "http://api:8000"
SAMPLE_DOCS_DIR = "/opt/airflow/data/raw"

# DAG default arguments
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

# Create DAG
dag = DAG(
    dag_id='simple_document_pipeline',
    default_args=default_args,
    description='Simplified MLOps pipeline using existing API',
    schedule_interval='@daily',
    start_date=datetime(2025, 11, 1),
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'simple', 'api-based'],
)


def check_api_health(**context):
    """Task 1: Check if API is healthy"""
    logger.info("Checking API health")
    
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=10)
        if response.status_code == 200:
            logger.info("âœ… API is healthy")
            return {"status": "healthy", "api_url": API_BASE_URL}
        else:
            raise Exception(f"API returned status {response.status_code}")
    except Exception as e:
        logger.error(f"âŒ API health check failed: {e}")
        raise


def list_pending_documents(**context):
    """Task 2: List documents available for processing"""
    logger.info("Listing pending documents")
    
    # For demo, we'll use sample documents
    documents = [
        "education-loan-agreement.pdf",
        "home-loan-agreement.pdf",
        "personal-loan-application.pdf"
    ]
    
    stats = {
        "total_documents": len(documents),
        "documents": documents
    }
    
    # Push to XCom
    context['ti'].xcom_push(key='document_list', value=stats)
    
    logger.info(f"Found {len(documents)} documents")
    return stats


def process_documents_batch(**context):
    """Task 3: Process documents through API"""
    logger.info("Processing documents through API")
    
    # Get document list from previous task
    doc_stats = context['ti'].xcom_pull(
        task_ids='list_pending_documents',
        key='document_list'
    )
    
    if not doc_stats:
        logger.warning("No documents to process")
        return {"processed": 0, "successful": 0, "failed": 0}
    
    results = {
        "processed": 0,
        "successful": 0,
        "failed": 0,
        "details": []
    }
    
    # Note: This is a demonstration. In production, you would:
    # 1. Read actual files from storage
    # 2. Send them to the API
    # 3. Track the job IDs
    
    logger.info(f"Would process {doc_stats['total_documents']} documents via API")
    logger.info("In production, use /api/v1/batch-upload endpoint")
    
    # Simulate processing
    results["processed"] = doc_stats['total_documents']
    results["successful"] = doc_stats['total_documents']
    
    context['ti'].xcom_push(key='processing_results', value=results)
    
    return results


def validate_results(**context):
    """Task 4: Validate processing results"""
    logger.info("Validating processing results")
    
    results = context['ti'].xcom_pull(
        task_ids='process_documents_batch',
        key='processing_results'
    )
    
    if not results:
        raise Exception("No processing results found")
    
    validation = {
        "total_processed": results.get("processed", 0),
        "success_rate": (results.get("successful", 0) / max(results.get("processed", 1), 1)) * 100,
        "status": "passed" if results.get("successful", 0) > 0 else "failed"
    }
    
    logger.info(f"Validation: {validation['success_rate']:.1f}% success rate")
    
    context['ti'].xcom_push(key='validation_results', value=validation)
    
    return validation


def generate_pipeline_report(**context):
    """Task 5: Generate final pipeline report"""
    logger.info("Generating pipeline report")
    
    # Get all results from previous tasks
    api_health = context['ti'].xcom_pull(task_ids='check_api_health')
    doc_stats = context['ti'].xcom_pull(task_ids='list_pending_documents', key='document_list')
    proc_results = context['ti'].xcom_pull(task_ids='process_documents_batch', key='processing_results')
    validation = context['ti'].xcom_pull(task_ids='validate_results', key='validation_results')
    
    report = {
        "pipeline_run_date": datetime.now().isoformat(),
        "api_status": api_health,
        "documents_found": doc_stats.get("total_documents", 0) if doc_stats else 0,
        "documents_processed": proc_results.get("processed", 0) if proc_results else 0,
        "success_rate": validation.get("success_rate", 0) if validation else 0,
        "status": "completed"
    }
    
    logger.info("=" * 80)
    logger.info("PIPELINE EXECUTION REPORT")
    logger.info("=" * 80)
    logger.info(f"Run Date: {report['pipeline_run_date']}")
    logger.info(f"API Status: {report['api_status']}")
    logger.info(f"Documents Found: {report['documents_found']}")
    logger.info(f"Documents Processed: {report['documents_processed']}")
    logger.info(f"Success Rate: {report['success_rate']:.1f}%")
    logger.info(f"Status: {report['status']}")
    logger.info("=" * 80)
    
    return report


# Define tasks
task1 = PythonOperator(
    task_id='check_api_health',
    python_callable=check_api_health,
    dag=dag,
)

task2 = PythonOperator(
    task_id='list_pending_documents',
    python_callable=list_pending_documents,
    dag=dag,
)

task3 = PythonOperator(
    task_id='process_documents_batch',
    python_callable=process_documents_batch,
    dag=dag,
)

task4 = PythonOperator(
    task_id='validate_results',
    python_callable=validate_results,
    dag=dag,
)

task5 = PythonOperator(
    task_id='generate_pipeline_report',
    python_callable=generate_pipeline_report,
    dag=dag,
)

# Define task dependencies
task1 >> task2 >> task3 >> task4 >> task5
